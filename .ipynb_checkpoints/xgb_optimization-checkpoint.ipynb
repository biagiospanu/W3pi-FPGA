{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b360fe0f-57b4-46d1-996a-22a441646dd9",
   "metadata": {},
   "source": [
    "### XGB Training and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5316c4-2f78-4076-a922-2a4879be81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Optimize XGB based on AUC ------------\n",
    "if doOptimization:\n",
    "    # Parse max depth\n",
    "    parser = argparse.ArgumentParser('Simple xgboost training.')\n",
    "    parser.add_argument('-md', '--md', type=int, help='BDT max depth')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"Doing Optimization for maxDepth \", args.md)\n",
    "\n",
    "    # Extract features (x) and labels (y)\n",
    "    x_test  = df_test [ FEATURES ].copy()\n",
    "    x_train = df_train[ FEATURES ].copy()\n",
    "    y_test  = df_test ['class']\n",
    "    y_train = df_train['class']\n",
    "\n",
    "    # Define DMatrix for training and testing\n",
    "    dtest_clf  = xgb.DMatrix(x_test , y_test , enable_categorical=True)\n",
    "    dtrain_clf = xgb.DMatrix(x_train, y_train, enable_categorical=True)\n",
    "\n",
    "    # Reduced grid  args.md\n",
    "    max_depth        = np.arange(args.md, args.md+1, 1)\n",
    "    min_child_weight = np.array([2,3,4,5])\n",
    "    subsample        = np.array([0.8, 0.9, 1])\n",
    "    colsample_bytree = np.array([0.8, 0.9, 1])\n",
    "    eta              = np.array([0.1])\n",
    "\n",
    "    # Build np arrays to fill the grid\n",
    "    # Methods:\n",
    "    #  - tile()   repeats the array N times\n",
    "    #  - repeat() repeats the individual elements\n",
    "    # Algo:\n",
    "    #  - repeat -> times the params below\n",
    "    #  - tile   -> times the params above\n",
    "    max_depths        = np.repeat(max_depth         , len(min_child_weight)*len(subsample)*len(colsample_bytree)*len(eta)      )\n",
    "    min_child_weights = np.repeat(min_child_weight  , len(subsample)*len(colsample_bytree)*len(eta)                            )\n",
    "    min_child_weights = np.tile  (min_child_weights , len(max_depth)                                                           )\n",
    "    subsamples        = np.repeat(subsample         , len(colsample_bytree)*len(eta)                                           )\n",
    "    subsamples        = np.tile  (subsamples        , len(max_depth)*len(min_child_weight)                                     )\n",
    "    colsample_bytrees = np.repeat(colsample_bytree  , len(eta)                                                                 )\n",
    "    colsample_bytrees = np.tile  (colsample_bytrees , len(max_depth)*len(min_child_weight)*len(subsample)                      )\n",
    "    etas              = np.tile  (eta               , len(max_depth)*len(min_child_weight)*len(subsample)*len(colsample_bytree))\n",
    "\n",
    "    # Define the grid as Pandas DF\n",
    "    grid = pd.DataFrame(\n",
    "        {\n",
    "            'max_depth'        : list(max_depths),\n",
    "            'min_child_weight' : list(min_child_weights),\n",
    "            'subsample'        : list(subsamples),\n",
    "            'colsample_bytree' : list(colsample_bytrees),\n",
    "            'eta'              : list(etas)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print points to be scanned\n",
    "    print('Probing a grid of shape:', grid.shape)\n",
    "    print('- Parameters:')\n",
    "    print('  - max_depth        :', max_depth)\n",
    "    print('  - min_child_weight :', min_child_weight)\n",
    "    print('  - subsample        :', subsample)\n",
    "    print('  - colsample_bytree :', colsample_bytree)\n",
    "    print('  - eta              :', eta)\n",
    "\n",
    "    # Define npoints just for printing progress of optimization\n",
    "    npoints = int(grid.shape[0]/8)\n",
    "    if npoints < 10: npoints = 1\n",
    "\n",
    "    # Function to apply the xgb CrossValidation for each grid point\n",
    "    def fit(x):\n",
    "        # Set parameters for each grid point\n",
    "        # https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster\n",
    "        params = {\n",
    "            'booster'          : 'gbtree',\n",
    "            'objective'        : 'binary:logistic',\n",
    "            'eval_metric'      : 'auc',\n",
    "            'tree_method'      : 'hist',\n",
    "            'sampling_method'  : 'uniform',\n",
    "            'max_depth'        : int(x[0]),\n",
    "            'min_child_weight' : int(x[1]),\n",
    "            'subsample'        : x[2],\n",
    "            'colsample_bytree' : x[3],\n",
    "            'eta'              : x[4],\n",
    "        }\n",
    "\n",
    "        # Run CrossValidation for each grid-point\n",
    "        # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.cv\n",
    "        xgb_cv = xgb.cv(\n",
    "            dtrain                = dtrain_clf,\n",
    "            params                = params, \n",
    "            nfold                 = 5,\n",
    "            metrics               = 'auc',\n",
    "            seed                  = 42,\n",
    "            num_boost_round       = 999, # set large value\n",
    "            early_stopping_rounds = 10   # let early stopping select the round (i.e. the number of trees)\n",
    "        )\n",
    "\n",
    "        # Return values of from best round of boosting\n",
    "        best_round = np.array([xgb_cv['test-auc-mean'].argmax()])\n",
    "        res = np.concatenate((best_round, xgb_cv[-1:].values[0]), axis=None)\n",
    "        if int(x._name) % npoints == 0:\n",
    "            print('- Done Grid point:', x._name, 'Params:', x['max_depth'], x['min_child_weight'], x['subsample'], x['colsample_bytree'], x['eta'])\n",
    "            print('    nTrees:', res[0], ' Test AUC:', res[3])\n",
    "        return res\n",
    "\n",
    "    # Extend the grid to include xgb results for each point\n",
    "    print(\"Start Grid Search:\", datetime.now())\n",
    "    grid[['rounds', 'train-auc-mean', 'train-auc-std', 'test-auc-mean', 'test-auc-std']] = grid.apply(\n",
    "        fit,                   # Fuction to be applied\n",
    "        axis = 1,              # Apply to each row\n",
    "        result_type = 'expand' # List-like results will be turned into columns\n",
    "    )\n",
    "    print(\"End Grid Search:\", datetime.now())\n",
    "\n",
    "    # Print model with highest test-AUC\n",
    "    print('** test-auc-mean **\\n', grid.iloc[grid['test-auc-mean'].idxmax()])\n",
    "\n",
    "\n",
    "# ------------ Retrain model with optimized parameters ------------\n",
    "if doFinalFit:\n",
    "    print(\"Doint final fit\")\n",
    "\n",
    "    # Extract features (x) and labels (y)\n",
    "    x_test  = df_test [ FEATURES ].copy()\n",
    "    x_train = df_train[ FEATURES ].copy()\n",
    "    y_test  = df_test ['class']\n",
    "    y_train = df_train['class']\n",
    "\n",
    "    # Define DMatrix for training and testing\n",
    "    dtest_clf  = xgb.DMatrix(x_test , y_test , enable_categorical=True)\n",
    "    dtrain_clf = xgb.DMatrix(x_train, y_train, enable_categorical=True)\n",
    "\n",
    "    # Best params from optimization\n",
    "    print(\"Using best parameters:\")\n",
    "    params = {\n",
    "        'max_depth'       : 3,\n",
    "        'min_child_weight': 5,\n",
    "        'subsample'       : 1,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'eta'             : 0.1,\n",
    "        'eval_metric'     : 'auc',\n",
    "        'objective'       : 'binary:logistic'\n",
    "    }\n",
    "    print(params)\n",
    "\n",
    "    # Define large early stopping callback returning best model\n",
    "    early_stop = xgb.callback.EarlyStopping(rounds=10, save_best=True)\n",
    "\n",
    "    # Train classifier\n",
    "    xgb_classifier = xgb.train(\n",
    "        params,\n",
    "        dtrain_clf,\n",
    "        evals=[(dtest_clf, \"Test\")],\n",
    "        num_boost_round=999,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    # Classifier name\n",
    "    outname = 'updated_results/3_finalFit/xgbV175_binary_featV8_finalFit_v1.json'\n",
    "\n",
    "    # Save model and config\n",
    "    xgb_classifier.save_model(outname)\n",
    "    with open(outname.replace('.json','_cfg.json'), 'w', encoding='utf-8') as fout:\n",
    "        json.dump(xgb_classifier.save_config(), fout, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Run prediction\n",
    "    pred = xgb_classifier.predict(dtest_clf)\n",
    "\n",
    "    # Print model results\n",
    "    print('  - Model          :', outname)\n",
    "    print('    ROC AUC score  :', sk.metrics.roc_auc_score (y_test, pred))\n",
    "    print('    nTrees         :', xgb_classifier.num_boosted_rounds())\n",
    "\n",
    "    # Feature importances\n",
    "    f_gain   = xgb_classifier.get_score(importance_type=\"gain\"  )\n",
    "    f_weight = xgb_classifier.get_score(importance_type=\"weight\")\n",
    "    f_cover  = xgb_classifier.get_score(importance_type=\"cover\" )\n",
    "\n",
    "    k_gain   = list(f_gain  .keys()  )\n",
    "    k_weight = list(f_weight.keys()  )\n",
    "    k_cover  = list(f_cover .keys()  )\n",
    "    v_gain   = list(f_gain  .values())\n",
    "    v_weight = list(f_weight.values())\n",
    "    v_cover  = list(f_cover .values())\n",
    "\n",
    "    data_gain   = pd.DataFrame(data=v_gain  , index=k_gain  , columns=[\"score\"]).sort_values(by=\"score\", ascending=False)\n",
    "    data_weight = pd.DataFrame(data=v_weight, index=k_weight, columns=[\"score\"]).sort_values(by=\"score\", ascending=False)\n",
    "    data_cover  = pd.DataFrame(data=v_cover , index=k_cover , columns=[\"score\"]).sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7), dpi=100)\n",
    "    data_gain.nlargest(40, columns=\"score\").plot(kind='barh')\n",
    "    plt.title('XGB Feature Importance by Gain')\n",
    "    plt.xlabel('Score')\n",
    "    plt.savefig(outname.replace('.json', '_FeatImp_Gain.png'))\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7), dpi=100)\n",
    "    data_weight.nlargest(40, columns=\"score\").plot(kind='barh')\n",
    "    plt.title('XGB Feature Importance by Weight')\n",
    "    plt.xlabel('Score')\n",
    "    plt.savefig(outname.replace('.json', '_FeatImp_Weight.png'))\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7), dpi=100)\n",
    "    data_cover.nlargest(40, columns=\"score\").plot(kind='barh')\n",
    "    plt.title('XGB Feature Importance by Cover')\n",
    "    plt.xlabel('Score')\n",
    "    plt.savefig(outname.replace('.json', '_FeatImp_Cover.png'))\n",
    "    plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (fpga-ml)",
   "language": "python",
   "name": "fpga-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
